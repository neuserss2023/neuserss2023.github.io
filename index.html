<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Addtional Experimental Results from Authors' Response">
    <meta name="author"
          content="Author Names Omitted for Anonymous Review. Paper-ID [130]">

    <title>Experiment Results from Authors' Response</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Additional Experimental Results from Authors' Response</h2>
    <hr>
    <p class="authors">
        Author Names Omitted for Anonymous Review. Paper-ID [130]
    </p>
</div>

<div class="container">
    <p>We thank all reviewers for their comments. As part of the authors' response to reviewers, this webpage provides
        additional results on the following:<br>
        <b>(1)</b> SLAM results with a synthetic non-planar object layout setting with diverse object orientations.
        <br>
        <b>(2)</b> Comparison of the chosen EM-Fusion baseline results on our real-world sequences.</p>
    <div class="section">
        <h3>I. Experiment: Non-planar Object Layout with Various Orientations</h3>
        <br>
        <h4> A. Video for the Non-planar Data</h4>
        <br>
        <div class="col justify-content-center text-center">
            <video width="90%" controls playsinline="" loop="" preload="">
                <source src="figures/gt_video.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <br>
    <p>
        The above video showcases scene and trajectory data featuring a <b>non-planar, randomly oriented</b> object
        setup. The sequence
        is utilized to evaluate the effectiveness of the proposed NeuSE-based object SLAM strategy. We found that
        existing off-the-shelf semantic segmentors could not robustly detect non-upright posed mugs and bottles in
        real video and thus conducted our experiment in simulation.
        <br>
        Following the previous camera trajectory and tabletop settings outlined in Section V.A., which included various viewing angles,
        occlusion patterns, and object changes, we further randomly placed approximately 50% of the bottles and mugs (25
        out of 52 objects) in the previous synthetic sequence. This resulted in a "hilly" object layout, as shown in the
        video, with various mug and bottle orientations, making the sequence suitable for examining the proposed
        SE(3)-equivariance of NeuSE.
    </p>
    <hr>
    <h4> B. Results </h4>
    <br>
    <center><img src="figures/sim_orientation.png" style="width:90%"></center>
    <div class="section">
        <p>
            In the above (a)-(c) figures, we demonstrate the effectiveness of the
            proposed SLAM strategy in the presence of non-planar, randomly oriented objects both quantitatively and
            qualitatively, following the same "mug-only" and "all-object" experiment series as described in the paper.
            <br>
            <b>(a.1)-(a.2)</b>: Visualization of the trajectory estimates with color variation to indicate the ATE
            values. The proposed algorithm manages to give a qualitatively shape-consistent trajectory estimate
            from a non-planar object layout. <br>
            <b>(b)</b>: As shown in the "Non-planar" section of the table, in terms of adopted metrics, Absolute
            Trajectory Error
            (ATE) and translational Relative Pose Error (RPE), the proposed
            strategy
            turns out to perform better on the new randomly-oriented object sequence. This can be attributed to the
            following: <br>
            (1) Our
            training strategy involves generating training samples that provide a view of the object of interest
            from various angles, along with occlusion patterns from surrounding auxiliary objects. This enables the
            learning of the underlying geometric features of the category shape that are robust to new occlusion
            scenarios.
            <br>
            (2) The learned representation is SE(3)-equivariant by construction, meaning that knowledge learned from
            upright observations can also benefit processing laid down objects. This property further enhances the
            model's ability to generalize to new scenarios.
            <br>
            (3) The use of lying-down mugs in the dataset turns out to help reduce the shape ambiguity in the sequence,
            as their handles can be more frequently observed when pointing upwards than in the usual sideways direction.
            <br>

            <b>(c)</b>: Full scene reconstruction of all objects in the sequence along with the better "All Objects"
            trajectory estimate. This indicates NeuSE's reconstruction capability for objects with various orientations
            and the spatial consistency of the reconstruction and camera trajectory estimate.
        </p>
    </div>
    <hr>
    <div class="section">
        <h3>II. Comparison with EM-Fusion on Real-world Sequences</h3>
        <br>
        <h4>A. Choice of EM-Fusion as Baseline</h4>
        <p>
            We choose to evaluate the self-collected real-world sequences on <b>EM-Fusion</b>[1], an object-level SLAM
            approach that
            adopts local Signed Distance Function (SDF) object volumes to track moving objects while performing camera
            localization.
            <br>
            EM-Fusion is selected as one baseline to our RGB-D object SLAM approach for being an open-sourced object
            SLAM strategy that allows for direct evaluation without additional training. It is important to note that
            since
            Kimera [2], as suggested by Reviewer #3, operates with inertial (IMU) and visual data from mono and stereo
            cameras, it is of a different data modality than our approach (RGB-D). With no focus on inertial data in our
            algorithm, it is not considered a
            suitable baseline for our evaluation.
            <br>
            There are only few open-source object SLAM approaches available, and each approach may focus on
            handling a specific type of scene and relevant objects. For example, DSP-SLAM [3] looks at outdoor
            scenes and cars with monocular or stereo cameras and Lidar measurements, while CubeSLAM [4] and QuadricSLAM
            [5] are more suited for indoor scenes with a static
            environment. Hence, it is not easy to find an object SLAM approach that fully addresses the same problem as
            ours.
            <br>
            Despite not specifically focusing on long-term scene consistency, EM-Fusion is designed to handle "certain"
            dynamics, namely, motions of moving objects that occur in front of the camera. While also leveraging
            object-level
            information, EM-Fusion is then considered an appropriate object SLAM baseline to compare with our method.
            <br>

        </p>
        <hr>
        <h4>B. Results on Real-world Sequences</h4>
        <br>
        <center><img src="figures/emfusion.png" style="width:90%"></center>
        <br>
        <p>
            The above (a)-(c) figures depict the performance of EM-Fusion on the self-collected real-world sequences,
            where EM-Fusion produces subpar bumpy and drifted trajectory estimates compared to other experiments
            conducted in the paper.
            <br>
            <b>(a)</b>: Quantitative comparison in terms of Absolute Trajectory Error (ATE) of EM-Fusion (the last
            column) against results of all other
            variants presented by our paper. Regarding the ATE values, EM-Fusion performs much worse than its NeuSE- or
            ORB- based counterparts.
            <br>
            <b>(b.1)-(b.2)</b>: Visualization of EM-Fusion trajectory estimates on the 4-Round and Triple-Infinity loop.
            The darker color parts, indicating obvious drift around turning corner regions, showcase EM-Fusion's
            incapability in dealing with larger
            multi-table-top settings and susceptibility to fast rotation motion.
            <br>
            With <b>(a)</b> and <b>(b)</b>, we can see that EM-Fusion performs poorly on our real-world testing
            sequences, resulting in bumpy trajectory estimates with significant drift around table corners with sharper
            turning motion.
            <br>
            While it can deal with dynamics at frame intersections when our long-term scene
            layout changes, its poor performance is primarily attributed to the compromise between tracking accuracy and
            memory consumption and the accumulated drift when less object overlap is available. In order to maintain a
            finer SDF background volume (voxel size of 1 cm for a 5<sup>3</sup> m<sup>3</sup> space) for better tracking
            accuracy,
            the machine
            can easily run out of memory due to SDF volume instantiation. Therefore, EM-Fusion's effective continuous
            operating area is limited to roughly a tabletop size. To enable EM-Fusion to go through the whole
            sequence where the robot drives through five tabletops, the SDF volume resolution is slightly tuned, and the
            whole sequence is divided into several overlapping shorter sessions, from where the estimates are aligned
            and
            concatenated to obtain the final estimate.
            <br>
            <b>(c)</b>: Visualization of the <span style="color:green">estimation drift </span>against the <span
                style="color:orange">ground truth</span>. EM-Fusion suffers from heavier drift due to faster
            motions around the corners, and hence less object overlap for camera pose estimation.
            <br>
            In contrast to EM-Fusion, NeuSE represents objects as a compact point cloud surrogate (whose size is usually
            only a
            few hundred). NeuSE allows the full object shape to be encoded across viewing angles, making the
            algorithm less vulnerable to observation overlap during data association. Furthermore, the compact
            representation of objects in NeuSE makes it lightweight enough to function effectively within larger
            environments, making it a more practical option for real-world applications.
        </p>
        <hr>
        <h5>References</h5>
        <p>
            [1] Strecke, Michael, and Jorg Stuckler. "Em-fusion: Dynamic object-level slam with probabilistic data
            association." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019. <br>
            [2] Rosinol, Antoni, et al. "Kimera: an open-source library for real-time metric-semantic localization and
            mapping." 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020.
            [3] Wang, Jingwen, Martin Rünz, and Lourdes Agapito. "DSP-SLAM: object oriented SLAM with deep shape
            priors." 2021 International Conference on 3D Vision (3DV). IEEE, 2021. <br>
            [4] Yang, Shichao, and Sebastian Scherer. "Cubeslam: Monocular 3-d object slam." IEEE Transactions on
            Robotics 35.4 (2019): 925-938. <br>
            [5] Hosseinzadeh, Mehdi, et al. "Real-time monocular object-model aware sparse SLAM." 2019 International
            conference on robotics and automation (ICRA). IEEE, 2019.
        </p>


        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
                integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
                crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
                integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
                crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
                integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
                crossorigin="anonymous"></script>

</body>
</html>

