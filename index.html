<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Addtional Experimental Results from Authors' Response">
    <meta name="author"
          content="Author Names Omitted for Anonymous Review. Paper-ID [130]">

    <title>Experiment Results from Authors' Response</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Additional Experimental Results from Authors' Response</h2>
    <hr>
    <p class="authors">
        Author Names Omitted for Anonymous Review. Paper-ID [130]
    </p>
</div>

<div class="container">
    <p>We thank all reviewers for their comments. As part of
        authors' reponse to reviewers, this webpage provides additional results on:<br>
        <b>(1)</b> SLAM results with a synthetic non-planar object layout setting with diverse object orientations.
        <br>
        <b>(2)</b> Comparison results of the selected EM-Fusion baseline on our real-world sequence.</p>

    <div class="section">
        <h3>I. Experiment: Non-planar Object Layout with Various Orientations</h3>
        <br>
        <h4> A. Data Overview</h4>
        <br>
        <div class="col justify-content-center text-center">
            <video width="90%" controls playsinline="" loop="" preload="">
                <source src="figures/gt_video.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <br>
    <p>
        The video above showcases scene and trajectory data used to evaluate the effectiveness of the proposed
        NeuSE-based object SLAM strategy with a non-planar, randomly-oriented object setup. To demonstrate its
        effectiveness in principle and overcome the lack of <b>robust</b> off-the-shelf semantic segmentors for
        non-upright mugs and bottles,
        we conducted the experiment in simulation. In addition to our previous experiment settings in Section V.A, which
        featured
        various viewing angles, occlusion patterns, and object changes, we randomly laid down roughly 50% of the
        bottles and mugs (25 out of the 52 objects) in the previous synthetic sequence. This created a "hilly" object
        layout shown in the video. Note that we assume the use of sticky-tape-typed procedures to
        prevent lying-down mugs from rolling due to gravity so as to obtain a variety of mug orientations.
    </p>
    <hr>
    <h4> B. Results </h4>
    <br>
  <center><img src="figures/sim_orientation.png" style="width:90%"></center>
    <div class="section">
        <p>
            Following the same setting of running the experiment with only mugs and all objects to include bottles, we
            demonstrate the effectiveness of the
            proposed SLAM strategy in the face of randomly oriented objects both
            quantitatively and qualitatively. <br>
            <b>(a.1)-(a.2)</b> Visualization of the trajectory estimates with color variation to indicate the ATE
            values. The
            lighter color of the trajectory estimated with all objects further demonstrates the effectiveness of our
            design for leveraging ambiguous objects. <br>
            <b>(b)</b> As shown in the "Non Planar" section of the table, in terms of adopted metrics, Absolute Trajectory Error
            (ATE) and translational Relative Pose Error (RPE), the proposed
            strategy
            turns out to perform better on the new randomly-oriented object sequence. This can be attributed to: (1) Our
            training strategy (III.D) involves generating training samples that provide a view of the object of interest
            from various angles, along with occlusion patterns from surrounding auxiliary objects. This enables the
            learning of the underlying geometric features of the category shape that are robust to new occlusion
            scenarios.
            <br>
            (2) The learned representation is SE(3)-equivariant by construction,meaning that knowledge learned from
            upright observations can also benefit processing laid down objects. This property further enhances the
            model's ability to generalize to new scenarios.
            <br>
            (3) The use of lying-down mugs in the dataset turn out to help reduce the shape ambiguity in the sequence as
            their handles
            are more visible when pointing upwards than in the usual sideways direction.
            <br>

            <b>(c)</b> We perform full scene reconstruction of all objects that appear in the sequence and
            visualize it alongside the better "All Objects" trajectory estimate, demonstrating NeuSE's reconstruction
            capability for objects with various orientations, as well as the spatial consistency of the reconstruction
            and camera trajectory estimate.
        </p>
    </div>
    <hr>
    <div class="section">
        <h3>II. Comparison with EM-Fusion on Real-world Sequences</h3>
        <br>
        <h4>A. Choise of EM-Fusion as Baseline</h4>
        <p>
            We further choose to evaluate the self-collected real-world sequences on EM-Fusion[1], an object-level SLAM
            approach that
            adopts local Signed Distance Function (SDF) object volumes to track moving objects while performing camera
            localization.

            We have chosen EM-Fusion as our RGB-D object SLAM approach because it is relatively new and open-sourced,
            allowing for direct evaluation without the need for additional training. It is important to note that since
            Kimera [2], as suggested by Reviewer #3, operates with inertial (IMU) and visual data from mono and stereo
            cameras, it is of a different data modality than our approach (RGB-D). Therefore, it is not considered a
            suitable baseline for our evaluation.

            There are only few open-source object SLAM approaches available, and each approach may focus on
            handling a specific type of scene and relevant objects. For example, DSP-SLAM [3] looks at outdoor
            scenes and cars with monocular or stereo cameras and Lidar measurements, while CubeSLAM [4] and QuadricSLAM
            [5] are more suited for indoor scenes with a static
            environment. Hence, it is not easy to find an object SLAM approach that fully addresses the same problem as
            ours.

            Despite not specifically focusing on long-term scene consistency, EM-Fusion is designed to handle "certain"
            dynamics, namely, motions of moving objects that occur in front of the camera. By leveraging object-level
            information, EM-Fusion is considered an appropriate object SLAM baseline to compare with our method.
            <br>

        </p>
        <hr>
        <h4>B. Results on Real-world Sequences</h4>
        <br>
        <center><img src="figures/emfusion.png" style="width:90%"></center>
        <br>
        <p>
            <b>(a.1)-(a.2)</b> Visualization of EM-Fusion trajectory estimates on the 4-Round and Triple-Infinity loop.
            <br>
            <b>(b)</b> Quantitative comparison in terms of Absolute Trajectory Error (ATE) against results of all other
            variants presented by our paper. <br>

            The trajectory visualization in <b>(a)</b> reveals that EM-Fusion performs poorly on the real-world testing
            sequences, resulting in bumpy trajectory estimates with significant drift around table corners with sharper
            turning motion. While it is able to deal with dynamics at frame intersections when our long-term scene
            layout changes, its poor performance is primarily attributed to the compromise between tracking accuracy and
            memory consumption and the accumulated drift when less object overlap is available. In order to maintain a
            finer SDF background volume (voxel size of 1 cm for a 5m<sup>3</sup> space) for better tracking accuracy,
            the machine
            can easily run out of memory due to SDF volume instantiation. Therefore, EM-Fusion's effective continuous
            operating area is limited to roughly the size of a tabletop. To enable EM-Fusion to go through the whole
            sequence where the robot drives through five tabletops, the SDF volume resolution is slightly tuned, and the
            whole sequence is divided into several overlapping shorter sessions, from where the estimates are aligned
            and
            concatenated to obtain the final estimate.
            <br>
            <b>(c)</b> Visualization of the <span style="color:green"> estimation drift </span> against the <span
                style="color:orange">ground truth </span>. With faster turning motion hence less
            observation overlap on the object pixels to be projected for camera pose estimation, the less background
            pixels
            (compared to objects) then induce accumulated errors in the estimated trajectory, where bumpy drifty camera
            locations are observed as shown in <b>(c)</b> here. Compared to EM-Fusion, NeuSE represents objects in as a
            compact point cloud surrogate (usually in the order of hundreds) and encodes full object shape across
            viewing
            angles. This then makes us approach less vulnerable to observation overlap for data association and are
            lightweight enough to function within a larger environment.
        </p>
        <hr>
        <h5>References</h5>
        <p>
            [1] Strecke, Michael, and Jorg Stuckler. "Em-fusion: Dynamic object-level slam with probabilistic data
            association." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019. <br>
            [2] Rosinol, Antoni, et al. "Kimera: an open-source library for real-time metric-semantic localization and
            mapping." 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020.
            [3] Wang, Jingwen, Martin RÃ¼nz, and Lourdes Agapito. "DSP-SLAM: object oriented SLAM with deep shape
            priors." 2021 International Conference on 3D Vision (3DV). IEEE, 2021. <br>
            [4] Yang, Shichao, and Sebastian Scherer. "Cubeslam: Monocular 3-d object slam." IEEE Transactions on
            Robotics 35.4 (2019): 925-938. <br>
            [5] Hosseinzadeh, Mehdi, et al. "Real-time monocular object-model aware sparse SLAM." 2019 International
            conference on robotics and automation (ICRA). IEEE, 2019.
        </p>


        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
                integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
                crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
                integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
                crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
                integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
                crossorigin="anonymous"></script>

</body>
</html>

