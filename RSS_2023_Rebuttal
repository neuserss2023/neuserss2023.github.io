We thank all the reviewers for their constructive feedback and please see below for our response.

Meta Reviewer #2:
2&5.  Please check our website for the new experiments concerning non-planar object layout and the comparison on the real-world data with EM-Fusion as an object SLAM baseline dealing with dynamic objects.

Reviewer #1:
2. (1) The proposed NeuSE representation owns its transferable generalization capability from training on geometric point cloud partial observations. By seeing a number of different object shape models within the category (e.g., from ShapeNet [1]), the model is able to capture the common underlying geometric characteristics of the category, which is thus less affected from common sim2real gaps due to such as photometric difference and can be straightforwardly transferred into real-world instances. (2) As shown in Fig.2 in the paper, we obtain instance segmentation masks instead of bounding boxes from Detectron2, which can provide relatively accurate contours of the object and hence good partial point clouds. During training, we also add minor disturbance to the synthetic training sample to develop certain robustness to real-world depth sensing noises. 

3. (1) Compared to traditional iterative ICP that requires a good initialization, the SE(3) equivariance of the proposed representation is the novelty that distinguishes us from traditional methods so as to directly arrive at the final SE(3) camera pose constraints that are amenable to general SLAM backend optimization. We also have adopted the idea of bundle adjustment in our backend optimization, which is realized in the short-range sliding window odometry constraints and long-range loop closing (Section IV.C).
(2)  One of the failure cases could be when the obtained point cloud is too sparse to be anywhere close to the point cloud density seen during training, and that is why we only use partial object segmentations obtained from within a certain distance. For the treatment of  objects with geometric ambiguity, as the traditional absolute object pose (w.r.t. a reference) is ill-defined, the common training loss for enforcing the pose prediction to approach the single correct object pose is therefore not suitable. We therefore instead propose the treatment of evaluating transformed shape similarity instead of absolute pose differences to implicitly learn the underlying pose distribution, which is proved to be effective by the experiments in the paper as well as the new results from the non-planar object layout setting suggested in the review.

5. We develop a novel SE(3)-equivariant object representation that contributes to robust robot spatial perception, so we feel it pertains more to a science paper.

Reviewer #2
2. L1: Please check the website Section I for experiment results concerning mugs and bottles of both upright and lying-down with various orientations.
L2: (1) If scene configuration is about the categories of objects, our method is like most (if not all) learning-based methods that would work the best on the object categories previously seen during training, while being category-level enables the representation to generalize better than many learning-based approaches that demand exactly the same instance. Just like humans gradually expand their horizons, our method is also extendable to more object categories (as mentioned in the last line of Section V in the paper). Here, we pick the mug and bottle category as demonstration mostly for their ubiquity in our daily lives and their perfect geometric properties of examining our treatment of ambiguous and unambiguous objects.  We might also need to roughly know the ambiguity pattern of the objects, e.g., axial/cubical symmetry, based on which we sample around the symmetry axis to facilitate learning the pose distribution, but as this is only done in simulation for training, it will not incur consider human labor to obtain these samples. 
(2)  While not demanding knowledge about the objects, rendering-based approaches revolve around photometric losses and thus may need steady lighting environment for robust functioning and the availability of real-world data for training. Compared to rendering-based approaches, our method learns solely from object geometry and is hence able to transfer well to the real-world even trained fully simulation, while simulation also enables to expand to generate more training data with low labor cost. (要加光照的实验吗）

L3. (1) Our approach should be scalable to larger scenes. Just as a larger point cloud or voxel grid is needed to store the information of the expanding environment, as long as there are objects in sights to be processed, the proposed approach, as claimed in the paper, can be standalone or readily integrable to other SLAM pipelines for better localization and object-centric mapping ability.  As the system stores objects in its NeuSE latent code, which is of the shape of a N×3 point cloud (tested N=128/512 for our project and showed good performance) consisting of hundreds of points, it is actually of similar performance yet more efficient to store this point cloud surrogate of several hundred point size than a traditional real point cloud (10^4 points) or SDF volume (can be huge with fine resolution) for full characterization of object shape and pose. And we can flexibly adjust the query point density during rendering to satisfy our desired resolution to refrain from storing chunky point clouds or SDF volumes at the beginning. 
(2) This is mainly due to the considerations of explaining our change detection capability in a concise way rather than a limitation of our method, as it will not make sense to include two identical-shaped objects and then judge if the second instance just observed is the previous one that is moved here or a new instance newly added in. One potential solution is to record the instance location from reconstruction and store the two instances until if the robots returns to the earlier place and then update the latest situation. We think this adds extra engineering complexity but is of less technical value. We sincerely apologize if this triggers any confusion.


3. F1:  If [2] refers to NICE-SLAM, we guess this may not be the most fair comparison. Our tracking front-end operates at 28 FPS and is tested on a laptop with an Intel Core i7-9750H CPU and an Nvidia RTX 2070 GPU, including all the lightweight rendering efforts for change detection. With change detection module turned off, our front-end can reach a speed of 32-35 FPS fully satisfying the demand for real-time processing. On the contrary, NICE-SLAM reports to be running at 21 FPS on a desktop workstation with a 3.80GHz Intel i7-10700K CPU and an NVIDIA RTX 3090 GPU. Based on the reported values, our tracking method without certain engineering optimization already runs at least 10Hz faster than NICE-SLAM with a less powerful hardware. (说网络结构吗）
F2: The hyperparameter setup is relevant to properties about the scene, such as the scale of the scene, the objects, and the changes taken place. It can be adjusted accordingly, e.g., we may increase delta_prox if objects of interest change from bottles to wardrobe, and lower delta_e if all changes take place on the same tabletop instead  all around the room.  
5. C1: The unambiguous objects refer to objects with at least one viewing angle that has a unique object pose defined. For mugs with its handle obscured but unchanged, they will not contribute to the current frame pose estimation but will be sent to change detection module for further verification.  As described in IV.B and IV.D,  during data association those mugs will be shape consistent but partially inconsistent with the object in the library due to the wrong inferred transform as the latent code is guessing the correct handle orientation. Then with the camera pose inferred from other associated objects, change detection is performed to validate that the mug is unchanged  for its consistent neighboring object layout based on projected rendered object centers with the latest camera pose estimate (as is the case of Fig. 5(a)).
C2-4:Thank you for accurately capture these typos in C2-4, and in C4, the 3rd row is 3rd to 4th round.

Reviewer #3
2. (1) Our training process is fully in simulation with automatic availability of object-level mask, thus requires minimal human annotation labor.
(2) Our object latent code take the form of a point cloud usually with a size of several hundreds (128 and 512 have been tested to work well, with the corresponding network size of around 4.4MB to 17.4MB). Therefore, compared to traditional methods that represents the environment with point clouds of hundreds of thousands of points or voxel grid with millions of voxels, our approach should be relatively lightweight and hence more scalable to larger environment.
(3) Please refer to website Section II for our comparison with another object-level SLAM approach, EM-Fusion. Kimera is a SLAM methods operating on IMU with monocular or stereo image input swhile we are working with RGBD images, hence it may not be the best baseline for comparison.
(4) The accuracy improvement is relatively related to the scene properties and can be small sometimes for smaller, indoor scene, e.g., our data is collected in a highly controlled room equipped with motion capture system for getting ground truth trajectory. Besides, our approach not only shows improves on all different metrics, but significantly improve the ATE from 16 cm to 8 cm on the more challenging Triple-inifinity loop by running ORB-SLAM3 piecewisely with almost no loop closure, fully demonstrating the effectiveness of our approach in performing robust data association.

3. Please check website Section II for our comparison with EM-Fusion, while we want to say that there are quite limited object-level SLAM projects that are open-source and could be directly
evaluated without further training. Besides, different object SLAM approaches may look at only a specific type of scene along with relevant objects (e.g., list in the paper, DSP-SLAM[2] with outdoor
scenes and cars or CubeSLAM [3] and QuadricSLAM [4] with indoor scenes but static environment). Though focusing on highly dynamic motions in front the camera instead of long-term scene inconsistency, the intention of dealing with certain types of environment dynamics while leveraging object-level information make us find EM-Fusion a quite good baseline to compare with.

5. NeuSE is built on top of neural implicit representations with an extra SE(3)-equivariance property.  NeuSE, as a type of neural implicit representations, is trained on see various partial object point clouds of same category. In this way, they can learn to complete object shapes after seeing multiple samples from various viewing angles to nicely learn the underlying shared geometric structure of that category of objects. We train NeuSE in simulation with samples observed from a variety of directions (near & far, low & high places), while the objects are occluded by neighboring objects on a tabletop. In this way, NeuSE is then able to handle the various occlusion partterns quite well, as proved by our reconstruction of the synthetic and real-world data in the paper and the new reconstruction for non-planar object layout in website Section I.(c).


=================================================================================================================================================================
We feel Reviewer # 3 may not equip with the best expertise to review this paper. 
(1) The first, second, and last points they make in the second limitation question do not seem totally make sense to us.
(a) The full simulation-based training is one great advantage NeuSE to enable automatic availability of object annotations in many simulation engines (e.g., Pytorch), while the reviewer seems to be a bit unclear about it.
(b) As our strategy is object-centric and leverages the lightweight NeuSE formulation for encoding full object shape and pose with the size of just hundreds of points per object. Compared to  traditional voxel- or point-based approach for dense reconstruction of the object, our method will just preserve a fixed few hundred-point sized latent code and can render up to very high resolution depending the query point density at rendering time.
(c) The reivewer does not seem to be quite familiar with SLAM research progress. While Kimera being a SLAM approach targetting IMU and visual data modalities, our work adopts RGBD data so Kimera may not be the best baseline to compare from.  
(2) We are grateful for their suggestion on finding another baseline to test our robustness and hence choose EM-Fusion, though it focuses on moving objects in front of the camera so may not be of identical environment. With a focus on utilizing object information, we want to argue that for now few object-level SLAM are open-sourced and can be directly applied to testing without further training, and so it is even harder to find one that is closely related with our current work. Besides, recent SLAM systems has their own focus of application area , among open-sourced ones, DSP-SLAM [1] works with monocular or LiDAR+stereo camera for larger scene and objects (cars), and CubeSLAM [2] or Quadric SLAM [3] use simplified geometris for static environment with monocular cameras. Neural implicit object representations have recently emerged in many interesting fields such as reconstruction and manipulation. We hope the lack of abundant comparison to make for relatively new topics will not greatly hinder the chance of sharing interesting findings to the comminuty.



[2] Wang, Jingwen, Martin Rünz, and Lourdes Agapito. "DSP-SLAM: object oriented SLAM with deep shape priors." 2021 International Conference on 3D Vision (3DV). IEEE, 2021.
[3] Yang, Shichao, and Sebastian Scherer. "Cubeslam: Monocular 3-d object slam." IEEE Transactions on Robotics 35.4 (2019): 925-938.
[4] Hosseinzadeh, Mehdi, et al. "Real-time monocular object-model aware sparse SLAM." 2019 International conference on robotics and automation (ICRA). IEEE, 2019.

